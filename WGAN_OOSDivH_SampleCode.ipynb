{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:41.399590Z",
     "iopub.status.busy": "2022-07-07T05:50:41.399590Z",
     "iopub.status.idle": "2022-07-07T05:50:47.552786Z",
     "shell.execute_reply": "2022-07-07T05:50:47.552786Z",
     "shell.execute_reply.started": "2022-07-07T05:50:41.399590Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate, Layer, Dropout, RNN, LSTMCell, SimpleRNNCell,Bidirectional,LSTM,SimpleRNN\n",
    "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, Flatten,LeakyReLU,ReLU,Lambda \n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from functools import partial\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from keras import metrics, losses\n",
    "pd.options.display.max_rows = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:47.554780Z",
     "iopub.status.busy": "2022-07-07T05:50:47.554780Z",
     "iopub.status.idle": "2022-07-07T05:50:47.599630Z",
     "shell.execute_reply": "2022-07-07T05:50:47.599630Z",
     "shell.execute_reply.started": "2022-07-07T05:50:47.554780Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "gpu = tf.config.experimental.get_visible_devices('GPU')[0] # No GPU\n",
    "tf.config.experimental.set_memory_growth(device = gpu, enable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:47.600626Z",
     "iopub.status.busy": "2022-07-07T05:50:47.600626Z",
     "iopub.status.idle": "2022-07-07T05:50:49.842161Z",
     "shell.execute_reply": "2022-07-07T05:50:49.841164Z",
     "shell.execute_reply.started": "2022-07-07T05:50:47.600626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Load the Population dataset\n",
    "x_population = pd.read_csv('data/data_NHTSint_indiv.csv')\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Note that [5,10] in Age is not the first factors \n",
    "for col in x_population.columns:\n",
    "    x_population[col] = le.fit_transform(x_population[col])\n",
    "x_population = x_population.apply(lambda x: x.astype('category'))\n",
    "\n",
    "\n",
    "## Sample the 5% of samples\n",
    "_,x_sample = train_test_split(x_population,test_size=0.05,shuffle=True,random_state=1004)\n",
    "\n",
    "## Defining Catgroupsb\n",
    "cat_groups = [0]\n",
    "for col in x_population.columns:\n",
    "    cat_groups.append(x_population[col].nunique())\n",
    "    \n",
    "cat_groups = np.cumsum(cat_groups)\n",
    "cat_groups_n = len(cat_groups)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:49.844155Z",
     "iopub.status.busy": "2022-07-07T05:50:49.844155Z",
     "iopub.status.idle": "2022-07-07T05:50:52.576058Z",
     "shell.execute_reply": "2022-07-07T05:50:52.576058Z",
     "shell.execute_reply.started": "2022-07-07T05:50:49.844155Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Load pretrained bert model\n",
    "mlm_model = tf.keras.models.load_model(\n",
    "    \"MLM_Embed_Indiv5_New.h5\")\n",
    "# mlm_model = tf.keras.models.load_model(\n",
    "#     \"MLM_Embed_Indiv5.h5\")\n",
    "\n",
    "embedding_layers =  mlm_model.layers[13:26] ## Find a embedding layers from mlm_model.summary()\n",
    "\n",
    "def convert_to_embedding(samples):\n",
    "    samples_emb = []\n",
    "    for i in range(len(embedding_layers)):\n",
    "        emb_weight = embedding_layers[i].get_weights()[0]\n",
    "        trgt = samples[:,range(n_uni_col[i],n_uni_col[i+1])]\n",
    "        samples_emb.append(np.dot(trgt,emb_weight))\n",
    "    \n",
    "    return(np.concatenate(samples_emb,axis=1))\n",
    "\n",
    "def convert_to_embedding_tensor(y_pred):\n",
    "    samples_emb = []\n",
    "    for i in range(len(embedding_layers)):\n",
    "        emb_weight = embedding_layers[i].get_weights()[0]\n",
    "        trgt = y_pred[:,n_uni_col[i]:n_uni_col[i+1]]\n",
    "        samples_emb.append(tf.tensordot(trgt,emb_weight,1))\n",
    "    \n",
    "    return(tf.concat(samples_emb,axis=1))\n",
    "\n",
    "## Precision & Recall\n",
    "\n",
    "def compute_pairwise_distance(data_x, data_y=None):\n",
    "\n",
    "    if data_y is None:\n",
    "        data_y = data_x\n",
    "    dists = sklearn.metrics.pairwise_distances(\n",
    "        data_x, data_y, metric='l2', n_jobs=-1)\n",
    "    return dists\n",
    "\n",
    "def compute_pairwise_distance_tensor(A,B):\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)\n",
    "    # na as a row and nb as a co\"lumn vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    \n",
    "    # return pairwise euclidead difference matrix\n",
    "    dists = tf.sqrt(tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 1e-9))\n",
    "\n",
    "    return dists\n",
    "\n",
    "def compute_pairwise_hamming_distance(data_x, data_y=None):\n",
    "\n",
    "    if data_y is None:\n",
    "        data_y = data_x\n",
    "    dists = sklearn.metrics.pairwise_distances(\n",
    "        data_x, data_y, metric='hamming', n_jobs=-1)\n",
    "    return dists\n",
    "\n",
    "def compute_pairwise_hamming_distance_tensor(A,B):\n",
    "    na = tf.reduce_sum(tf.square(A), 1)\n",
    "    nb = tf.reduce_sum(tf.square(B), 1)\n",
    "    # na as a row and nb as a column vectors\n",
    "    na = tf.reshape(na, [-1, 1])\n",
    "    nb = tf.reshape(nb, [1, -1])\n",
    "    \n",
    "    # return pairwise euclidead difference matrix\n",
    "    hamming_dists = na - 2*tf.matmul(A, B, False, True) + nb\n",
    "\n",
    "    return hamming_dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:52.578052Z",
     "iopub.status.busy": "2022-07-07T05:50:52.577056Z",
     "iopub.status.idle": "2022-07-07T05:50:52.700649Z",
     "shell.execute_reply": "2022-07-07T05:50:52.700649Z",
     "shell.execute_reply.started": "2022-07-07T05:50:52.578052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Defining training hyperparameters\n",
    "x_train = pd.get_dummies(train_test_split(x_population,test_size=0.05,shuffle=True,random_state=1004)[1])\n",
    "\n",
    "from random import sample\n",
    "## Make Ground Truth & Test\n",
    "n_uni_col = [x_sample[i].nunique() for i in x_sample.columns]\n",
    "n_uni_col = [0]+n_uni_col\n",
    "n_uni_col = np.cumsum(n_uni_col)\n",
    "\n",
    "col_pop = x_train.columns.values.copy()\n",
    "for i in range(len(col_pop)):\n",
    "    col_pop[i] = col_pop[i].split('_', 1)[1]\n",
    "col_pop = col_pop.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:52.702643Z",
     "iopub.status.busy": "2022-07-07T05:50:52.701646Z",
     "iopub.status.idle": "2022-07-07T05:50:53.328558Z",
     "shell.execute_reply": "2022-07-07T05:50:53.328558Z",
     "shell.execute_reply.started": "2022-07-07T05:50:52.701646Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(x_population)\n",
    "x_sam_dum = enc.transform(x_sample).toarray()\n",
    "sam_embed = convert_to_embedding(x_sam_dum)\n",
    "sam_embed_tensor = tf.convert_to_tensor(sam_embed,dtype='float32')\n",
    "x_sam_dum_tensor = tf.convert_to_tensor(x_sam_dum,dtype='float32')\n",
    "#sam_embed_tensor = tf.constant(sam_embed,dtype='float32')\n",
    "#sam_embed_tensor = tf.keras.initializers.Constant(sam_embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:53.330551Z",
     "iopub.status.busy": "2022-07-07T05:50:53.329555Z",
     "iopub.status.idle": "2022-07-07T05:50:53.344505Z",
     "shell.execute_reply": "2022-07-07T05:50:53.344505Z",
     "shell.execute_reply.started": "2022-07-07T05:50:53.330551Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_generator():\n",
    "    noise = Input(shape=(latent_dim))\n",
    "    \n",
    "    h = Dense(intermediate_dim[0],activation='relu')(noise)\n",
    "    h = BatchNormalization()(h)\n",
    "    for num_layer in range(len(intermediate_dim)-1):\n",
    "        h = Dense(intermediate_dim[num_layer+1],activation='relu')(h)\n",
    "        h = BatchNormalization()(h)\n",
    "\n",
    "    cat_outputs = []\n",
    "    for i in x_sample.columns:\n",
    "        t = Dense(x_sample[i].nunique(),activation='softmax')(h)\n",
    "        cat_outputs.append(t)                                \n",
    "       \n",
    "    concat = Concatenate()(cat_outputs)\n",
    "    \n",
    "    g_model = Model(noise,concat)\n",
    "\n",
    "    return g_model\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:53.346498Z",
     "iopub.status.busy": "2022-07-07T05:50:53.345501Z",
     "iopub.status.idle": "2022-07-07T05:50:53.359455Z",
     "shell.execute_reply": "2022-07-07T05:50:53.359455Z",
     "shell.execute_reply.started": "2022-07-07T05:50:53.346498Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_critic():\n",
    "    \n",
    "    img = Input(shape=x_train.shape[1])\n",
    "    \n",
    "    h = Dense(intermediate_dim[0],activation=LeakyReLU(alpha=0.2))(img)\n",
    "    for num_layer in range(len(intermediate_dim)-1):\n",
    "        h = Dense(intermediate_dim[num_layer+1],activation=LeakyReLU(alpha=0.2))(h)\n",
    "\n",
    "    validity = Dense(1)(h)\n",
    "    \n",
    "    c_model = Model(inputs = img,outputs = validity)\n",
    "\n",
    "    return(c_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:53.361448Z",
     "iopub.status.busy": "2022-07-07T05:50:53.360451Z",
     "iopub.status.idle": "2022-07-07T05:50:53.375401Z",
     "shell.execute_reply": "2022-07-07T05:50:53.375401Z",
     "shell.execute_reply.started": "2022-07-07T05:50:53.361448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def RandomWeightedAverage(inputs):\n",
    "    alpha = K.random_uniform((BATCH_SIZE, 1))\n",
    "    #alpha = tf.random.normal([BATCH_SIZE, 1],0.0,1.0)\n",
    "    return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "        \n",
    "def gradient_penalty_loss(y_true, y_pred, averaged_samples):\n",
    "    \"\"\"\n",
    "    Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "    \"\"\"\n",
    "    gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "    # compute the euclidean norm by squaring ...\n",
    "    gradients_sqr = K.square(gradients)\n",
    "    #   ... summing over the rows ...\n",
    "    gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                              axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "    #   ... and sqrt\n",
    "    gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "    # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "    gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "    # return the mean as loss over all the batch samples\n",
    "    return K.mean(gradient_penalty)\n",
    "\n",
    "def OOS_loss(y_true,y_pred):\n",
    "    # Get the OOS loss\n",
    "    Hamming_distance = compute_pairwise_hamming_distance_tensor(y_pred,x_sam_dum_tensor)\n",
    "    Hamming_nearest_distance = tf.math.reduce_min(Hamming_distance,axis=1)\n",
    "    OOSloss = K.mean(Hamming_nearest_distance) \n",
    "    return OOSloss\n",
    "\n",
    "def MS_loss(y_true,y_pred):\n",
    "    # Get the OOS loss\n",
    "    img1, img2 = tf.split(y_pred,2,axis=0,name='image_split')\n",
    "    noise1, noise2 = tf.split(z_gen,2,axis=0,name='noise_split')\n",
    "    MSloss = tf.reduce_mean(tf.abs(img1-img2)) / tf.reduce_mean(tf.abs(noise1-noise2))\n",
    "    return -MSloss\n",
    "\n",
    "# # div loss\n",
    "def div_loss(y_true,y_pred):\n",
    "  \n",
    "    Hamming_distance = compute_pairwise_hamming_distance_tensor(y_pred,x_sam_dum_tensor)\n",
    "    Hamming_mean_distance = tf.math.reduce_mean(Hamming_distance,axis=1)\n",
    "    divloss = - K.mean(Hamming_mean_distance)\n",
    "    \n",
    "    return divloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:53.378392Z",
     "iopub.status.busy": "2022-07-07T05:50:53.378392Z",
     "iopub.status.idle": "2022-07-07T05:50:53.407296Z",
     "shell.execute_reply": "2022-07-07T05:50:53.407296Z",
     "shell.execute_reply.started": "2022-07-07T05:50:53.378392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wide_to_long(samples_pop):\n",
    "    resamples = []\n",
    "    for j in range(samples_pop.shape[0]):\n",
    "        if(type(samples_pop) is np.ndarray):\n",
    "            sam = samples_pop[j]\n",
    "        else:\n",
    "            sam = samples_pop.values[j]\n",
    "        resamples_row = []\n",
    "        for i in range(len(n_uni_col)-1):\n",
    "            idx = range(n_uni_col[i],n_uni_col[i+1])\n",
    "            resamples_row = np.append(resamples_row,np.random.choice(col_pop[idx],p=sam[idx],size=1))\n",
    "        resamples = np.concatenate((resamples,resamples_row),axis=0)\n",
    "    resamples = resamples.reshape(samples_pop.shape[0],len(n_uni_col)-1 )\n",
    "    resamples = pd.DataFrame(resamples,columns= x_sample.columns.to_list())\n",
    "    resamples = resamples.apply(lambda x: x.astype('category'))\n",
    "    return(resamples)\n",
    "\n",
    "def mean_JSD(samples,resamples):\n",
    "    Marg_JSD = []\n",
    "    for col in samples.columns:\n",
    "        resam = pd.value_counts(resamples[col]).sort_index()\n",
    "        sam = pd.value_counts(samples[col]).sort_index()\n",
    "        tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "        tab = tab.fillna(0)\n",
    "        Marg_JSD.append(jensenshannon(tab.iloc[:,0], tab.iloc[:,1]))\n",
    "     \n",
    "    bi_index = combinations(samples.columns,2)\n",
    "    bi_index = list(bi_index)\n",
    "    col1,col2 = bi_index[0]\n",
    "\n",
    "    Bi_JSD = []\n",
    "    for col1,col2 in bi_index:\n",
    "        resam = pd.DataFrame(pd.crosstab(resamples[col1],resamples[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "        sam = pd.DataFrame(pd.crosstab(samples[col1],samples[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "        tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "        tab = tab.fillna(0)\n",
    "        Bi_JSD.append(jensenshannon(tab.iloc[:,0], tab.iloc[:,1]))\n",
    "\n",
    "    return([Marg_JSD,Bi_JSD])\n",
    "\n",
    "def SRMSE(x_population,resamples):\n",
    "    \n",
    "    ## Aggregated Marginal distribution\n",
    "    sam_marg_cnt = []\n",
    "    resam_marg_cnt = []\n",
    "    for col in x_population.columns:\n",
    "        resam = pd.value_counts(resamples[col]).sort_index()\n",
    "        sam = pd.value_counts(x_population[col]).sort_index()\n",
    "        tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "        tab = tab.fillna(0)\n",
    "        sam_marg_cnt.append(tab.iloc[:,1].values/x_population.shape[0])\n",
    "        resam_marg_cnt.append(tab.iloc[:,0].values/resamples.shape[0])\n",
    "    \n",
    "    sam_marg_cnt = np.concatenate(sam_marg_cnt)\n",
    "    resam_marg_cnt = np.concatenate(resam_marg_cnt)\n",
    "    \n",
    "    ## Aggregated Bivariate distribution    \n",
    "    bi_index = combinations(x_population.columns,2)\n",
    "    bi_index = list(bi_index)\n",
    "    col1,col2 = bi_index[0]\n",
    "\n",
    "    sam_bi_cnt  = []\n",
    "    resam_bi_cnt = []\n",
    "    for col1,col2 in bi_index:\n",
    "        resam = pd.DataFrame(pd.crosstab(resamples[col1],resamples[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "        sam = pd.DataFrame(pd.crosstab(x_population[col1],x_population[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "        tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "        tab = tab.fillna(0)\n",
    "        sam_bi_cnt.append(tab.iloc[:,1].values/x_population.shape[0])\n",
    "        resam_bi_cnt.append(tab.iloc[:,0].values/resamples.shape[0])\n",
    "\n",
    "    sam_bi_cnt = np.concatenate(sam_bi_cnt)\n",
    "    resam_bi_cnt = np.concatenate(resam_bi_cnt)\n",
    "    \n",
    "    # SRMSE_mar\n",
    "    rmse_mar = np.linalg.norm(sam_marg_cnt - resam_marg_cnt) / np.sqrt(len(sam_marg_cnt))\n",
    "    ybar_mar = sam_marg_cnt.mean()\n",
    "    srmse_mar = rmse_mar / ybar_mar\n",
    "\n",
    "    # SRMSE_bi\n",
    "    rmse_bi = np.linalg.norm(sam_bi_cnt - resam_bi_cnt) / np.sqrt(len(sam_bi_cnt))\n",
    "    ybar_bi = sam_bi_cnt.mean()\n",
    "    srmse_bi = rmse_bi / ybar_bi\n",
    "    \n",
    "    return([srmse_mar,srmse_bi])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:53.409289Z",
     "iopub.status.busy": "2022-07-07T05:50:53.409289Z",
     "iopub.status.idle": "2022-07-07T05:50:53.423242Z",
     "shell.execute_reply": "2022-07-07T05:50:53.423242Z",
     "shell.execute_reply.started": "2022-07-07T05:50:53.409289Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\euijin\\AppData\\Local\\Temp\\ipykernel_16740\\2264731724.py:6: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  epochs = np.int(200*x_train.shape[0]*0.7/BATCH_SIZE)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "## Train\n",
    "BATCH_SIZE = 256\n",
    "epochs = np.int(200*x_train.shape[0]*0.7/BATCH_SIZE)\n",
    "current_epochs= 1 # Epoch start from\n",
    "sample_interval = 500 # Save checkpoint at every n epoch\n",
    "n_critic = 3\n",
    "losslog = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T05:50:53.425235Z",
     "iopub.status.busy": "2022-07-07T05:50:53.424239Z",
     "iopub.status.idle": "2022-07-07T08:15:20.378657Z",
     "shell.execute_reply": "2022-07-07T08:15:20.378657Z",
     "shell.execute_reply.started": "2022-07-07T05:50:53.425235Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\euijin\\.conda\\envs\\DGM\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "0 [D loss: 3.349824] [G loss: 3.043435] [OOS loss: -0.077983] [Div loss: 6.338749]\n",
      "500 [D loss: -0.991591] [G loss: 0.281251] [OOS loss: -0.288431] [Div loss: 1.244831]\n",
      "1000 [D loss: -0.760539] [G loss: 0.401900] [OOS loss: -0.046122] [Div loss: 1.004580]\n",
      "1500 [D loss: -0.607161] [G loss: 0.434072] [OOS loss: -0.008366] [Div loss: 0.994740]\n",
      "2000 [D loss: -0.615086] [G loss: 0.425990] [OOS loss: 0.021933] [Div loss: 0.919477]\n",
      "2500 [D loss: -0.496312] [G loss: 0.512406] [OOS loss: 0.173222] [Div loss: 0.791362]\n",
      "3000 [D loss: -0.491020] [G loss: 0.611334] [OOS loss: 0.272835] [Div loss: 0.789873]\n",
      "3500 [D loss: -0.515694] [G loss: 0.562926] [OOS loss: 0.234761] [Div loss: 0.769749]\n",
      "4000 [D loss: -0.501835] [G loss: 0.514392] [OOS loss: 0.128378] [Div loss: 0.885595]\n",
      "4500 [D loss: -0.503777] [G loss: 0.556013] [OOS loss: 0.200342] [Div loss: 0.825318]\n",
      "5000 [D loss: -0.446996] [G loss: 0.517726] [OOS loss: 0.173276] [Div loss: 0.803396]\n",
      "5500 [D loss: -0.479946] [G loss: 0.516207] [OOS loss: 0.176437] [Div loss: 0.794681]\n",
      "6000 [D loss: -0.423182] [G loss: 0.435025] [OOS loss: 0.104289] [Div loss: 0.777043]\n",
      "6500 [D loss: -0.500567] [G loss: 0.543685] [OOS loss: 0.172710] [Div loss: 0.857693]\n",
      "7000 [D loss: -0.350157] [G loss: 0.468755] [OOS loss: 0.089324] [Div loss: 0.874571]\n",
      "7500 [D loss: -0.422309] [G loss: 0.459988] [OOS loss: 0.122230] [Div loss: 0.791692]\n",
      "8000 [D loss: -0.358088] [G loss: 0.423915] [OOS loss: 0.041403] [Div loss: 0.880921]\n",
      "8500 [D loss: -0.386350] [G loss: 0.530916] [OOS loss: 0.171502] [Div loss: 0.835064]\n",
      "9000 [D loss: -0.322003] [G loss: 0.415960] [OOS loss: 0.046132] [Div loss: 0.855664]\n",
      "9500 [D loss: -0.429557] [G loss: 0.400460] [OOS loss: 0.079046] [Div loss: 0.759561]\n",
      "10000 [D loss: -0.404297] [G loss: 0.428380] [OOS loss: 0.082522] [Div loss: 0.807974]\n",
      "10500 [D loss: -0.365877] [G loss: 0.332259] [OOS loss: 0.014462] [Div loss: 0.752958]\n",
      "11000 [D loss: -0.361972] [G loss: 0.403541] [OOS loss: 0.040708] [Div loss: 0.842443]\n",
      "11500 [D loss: -0.303149] [G loss: 0.370979] [OOS loss: 0.034522] [Div loss: 0.789936]\n",
      "12000 [D loss: -0.265773] [G loss: 0.372136] [OOS loss: 0.012169] [Div loss: 0.836522]\n",
      "12500 [D loss: -0.275917] [G loss: 0.318358] [OOS loss: -0.012797] [Div loss: 0.780249]\n",
      "13000 [D loss: -0.412432] [G loss: 0.291481] [OOS loss: -0.005736] [Div loss: 0.711532]\n",
      "13500 [D loss: -0.378103] [G loss: 0.317735] [OOS loss: -0.030357] [Div loss: 0.813793]\n",
      "14000 [D loss: -0.318708] [G loss: 0.405617] [OOS loss: 0.012221] [Div loss: 0.904429]\n",
      "14500 [D loss: -0.267082] [G loss: 0.316500] [OOS loss: -0.039952] [Div loss: 0.830588]\n",
      "15000 [D loss: -0.312272] [G loss: 0.174568] [OOS loss: -0.177360] [Div loss: 0.821387]\n",
      "15500 [D loss: -0.371437] [G loss: 0.334687] [OOS loss: 0.002041] [Div loss: 0.782510]\n",
      "16000 [D loss: -0.274034] [G loss: 0.248538] [OOS loss: -0.094915] [Div loss: 0.804965]\n",
      "16500 [D loss: -0.305804] [G loss: 0.273577] [OOS loss: -0.040651] [Div loss: 0.746601]\n",
      "17000 [D loss: -0.282802] [G loss: 0.242993] [OOS loss: -0.055955] [Div loss: 0.715750]\n",
      "17500 [D loss: -0.236819] [G loss: 0.266128] [OOS loss: -0.059938] [Div loss: 0.770355]\n",
      "18000 [D loss: -0.342201] [G loss: 0.233242] [OOS loss: -0.072348] [Div loss: 0.728927]\n",
      "18500 [D loss: -0.342369] [G loss: 0.259896] [OOS loss: -0.028316] [Div loss: 0.694579]\n",
      "19000 [D loss: -0.302992] [G loss: 0.271302] [OOS loss: -0.074110] [Div loss: 0.809234]\n",
      "19500 [D loss: -0.266853] [G loss: 0.286222] [OOS loss: -0.055152] [Div loss: 0.800744]\n",
      "20000 [D loss: -0.333707] [G loss: 0.324523] [OOS loss: -0.019024] [Div loss: 0.805270]\n",
      "20500 [D loss: -0.255478] [G loss: 0.238543] [OOS loss: -0.107751] [Div loss: 0.811235]\n",
      "21000 [D loss: -0.289946] [G loss: 0.267103] [OOS loss: -0.084553] [Div loss: 0.821322]\n",
      "21500 [D loss: -0.226857] [G loss: 0.286319] [OOS loss: -0.093267] [Div loss: 0.877861]\n",
      "22000 [D loss: -0.267249] [G loss: 0.279134] [OOS loss: -0.059886] [Div loss: 0.795989]\n",
      "22500 [D loss: -0.264008] [G loss: 0.215715] [OOS loss: -0.144702] [Div loss: 0.838938]\n",
      "23000 [D loss: -0.320905] [G loss: 0.227317] [OOS loss: -0.160467] [Div loss: 0.894015]\n",
      "23500 [D loss: -0.436302] [G loss: 0.244338] [OOS loss: -0.071166] [Div loss: 0.749087]\n",
      "24000 [D loss: -0.279227] [G loss: 0.188164] [OOS loss: -0.191797] [Div loss: 0.878005]\n",
      "24500 [D loss: -0.282055] [G loss: 0.193586] [OOS loss: -0.181805] [Div loss: 0.869306]\n",
      "25000 [D loss: -0.297912] [G loss: 0.175032] [OOS loss: -0.138273] [Div loss: 0.744922]\n",
      "25500 [D loss: -0.277975] [G loss: 0.083708] [OOS loss: -0.245910] [Div loss: 0.777524]\n",
      "26000 [D loss: -0.318842] [G loss: 0.135077] [OOS loss: -0.161581] [Div loss: 0.712512]\n",
      "26500 [D loss: -0.253737] [G loss: 0.206681] [OOS loss: -0.130766] [Div loss: 0.794003]\n",
      "27000 [D loss: -0.303583] [G loss: 0.139812] [OOS loss: -0.167697] [Div loss: 0.733840]\n",
      "27500 [D loss: -0.326402] [G loss: 0.164207] [OOS loss: -0.204134] [Div loss: 0.854757]\n",
      "28000 [D loss: -0.293552] [G loss: 0.137821] [OOS loss: -0.190606] [Div loss: 0.775147]\n",
      "28500 [D loss: -0.233847] [G loss: 0.147407] [OOS loss: -0.178781] [Div loss: 0.771400]\n",
      "29000 [D loss: -0.244967] [G loss: 0.131133] [OOS loss: -0.216778] [Div loss: 0.814568]\n",
      "time : 1985.35343003273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\euijin\\.conda\\envs\\DGM\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 3.521802] [G loss: 3.062373] [OOS loss: -0.016044] [Div loss: 6.276444]\n",
      "500 [D loss: -0.813097] [G loss: 0.173531] [OOS loss: -0.431148] [Div loss: 1.340166]\n",
      "1000 [D loss: -0.704419] [G loss: 0.514697] [OOS loss: 0.041543] [Div loss: 1.081754]\n",
      "1500 [D loss: -0.644966] [G loss: 0.726431] [OOS loss: 0.346714] [Div loss: 0.897425]\n",
      "2000 [D loss: -0.709272] [G loss: 0.870713] [OOS loss: 0.475011] [Div loss: 0.930156]\n",
      "2500 [D loss: -0.521706] [G loss: 0.847971] [OOS loss: 0.429199] [Div loss: 0.978607]\n",
      "3000 [D loss: -0.552467] [G loss: 0.837238] [OOS loss: 0.481047] [Div loss: 0.853986]\n",
      "3500 [D loss: -0.505551] [G loss: 0.761891] [OOS loss: 0.425222] [Div loss: 0.815237]\n",
      "4000 [D loss: -0.469479] [G loss: 0.668537] [OOS loss: 0.303108] [Div loss: 0.873402]\n",
      "4500 [D loss: -0.457403] [G loss: 0.657746] [OOS loss: 0.312088] [Div loss: 0.834133]\n",
      "5000 [D loss: -0.372501] [G loss: 0.656981] [OOS loss: 0.305697] [Div loss: 0.846584]\n",
      "5500 [D loss: -0.469975] [G loss: 0.622593] [OOS loss: 0.261160] [Div loss: 0.866786]\n",
      "6000 [D loss: -0.378757] [G loss: 0.600912] [OOS loss: 0.244135] [Div loss: 0.857811]\n",
      "6500 [D loss: -0.383087] [G loss: 0.592337] [OOS loss: 0.242242] [Div loss: 0.845680]\n",
      "7000 [D loss: -0.386328] [G loss: 0.513954] [OOS loss: 0.162940] [Div loss: 0.847386]\n",
      "7500 [D loss: -0.405724] [G loss: 0.578612] [OOS loss: 0.223591] [Div loss: 0.855403]\n",
      "8000 [D loss: -0.344722] [G loss: 0.490035] [OOS loss: 0.153564] [Div loss: 0.818884]\n",
      "8500 [D loss: -0.482165] [G loss: 0.543858] [OOS loss: 0.212548] [Div loss: 0.807509]\n",
      "9000 [D loss: -0.399003] [G loss: 0.560570] [OOS loss: 0.179689] [Div loss: 0.907014]\n",
      "9500 [D loss: -0.368813] [G loss: 0.425856] [OOS loss: 0.068020] [Div loss: 0.862074]\n",
      "10000 [D loss: -0.327662] [G loss: 0.396630] [OOS loss: 0.033039] [Div loss: 0.873115]\n",
      "10500 [D loss: -0.374187] [G loss: 0.406509] [OOS loss: 0.106007] [Div loss: 0.747468]\n",
      "11000 [D loss: -0.288500] [G loss: 0.406813] [OOS loss: 0.077509] [Div loss: 0.805452]\n",
      "11500 [D loss: -0.345544] [G loss: 0.344377] [OOS loss: 0.027903] [Div loss: 0.779025]\n",
      "12000 [D loss: -0.422254] [G loss: 0.440775] [OOS loss: 0.111420] [Div loss: 0.804553]\n",
      "12500 [D loss: -0.343591] [G loss: 0.371217] [OOS loss: 0.006449] [Div loss: 0.876026]\n",
      "13000 [D loss: -0.350742] [G loss: 0.415083] [OOS loss: 0.079183] [Div loss: 0.818753]\n",
      "13500 [D loss: -0.313247] [G loss: 0.479720] [OOS loss: 0.165694] [Div loss: 0.774299]\n",
      "14000 [D loss: -0.439377] [G loss: 0.404673] [OOS loss: 0.095946] [Div loss: 0.763890]\n",
      "14500 [D loss: -0.322485] [G loss: 0.421068] [OOS loss: 0.143422] [Div loss: 0.701918]\n",
      "15000 [D loss: -0.334915] [G loss: 0.373867] [OOS loss: 0.085215] [Div loss: 0.724262]\n",
      "15500 [D loss: -0.375785] [G loss: 0.338999] [OOS loss: 0.005012] [Div loss: 0.814505]\n",
      "16000 [D loss: -0.327618] [G loss: 0.444779] [OOS loss: 0.130311] [Div loss: 0.776709]\n",
      "16500 [D loss: -0.412971] [G loss: 0.403004] [OOS loss: 0.129270] [Div loss: 0.694288]\n",
      "17000 [D loss: -0.359330] [G loss: 0.398048] [OOS loss: 0.075676] [Div loss: 0.791543]\n",
      "17500 [D loss: -0.315073] [G loss: 0.341365] [OOS loss: 0.008479] [Div loss: 0.813159]\n",
      "18000 [D loss: -0.328348] [G loss: 0.419194] [OOS loss: 0.072494] [Div loss: 0.840403]\n",
      "18500 [D loss: -0.324938] [G loss: 0.429191] [OOS loss: 0.162824] [Div loss: 0.679663]\n",
      "19000 [D loss: -0.361360] [G loss: 0.429202] [OOS loss: 0.075994] [Div loss: 0.853873]\n",
      "19500 [D loss: -0.367881] [G loss: 0.419990] [OOS loss: 0.114933] [Div loss: 0.757802]\n",
      "20000 [D loss: -0.307923] [G loss: 0.375396] [OOS loss: 0.091465] [Div loss: 0.715582]\n",
      "20500 [D loss: -0.318628] [G loss: 0.458529] [OOS loss: 0.168898] [Div loss: 0.727682]\n",
      "21000 [D loss: -0.340844] [G loss: 0.422062] [OOS loss: 0.106001] [Div loss: 0.779527]\n",
      "21500 [D loss: -0.317011] [G loss: 0.414837] [OOS loss: 0.046329] [Div loss: 0.884570]\n",
      "22000 [D loss: -0.380686] [G loss: 0.428314] [OOS loss: 0.134520] [Div loss: 0.734913]\n",
      "22500 [D loss: -0.190427] [G loss: 0.399202] [OOS loss: 0.078508] [Div loss: 0.789916]\n",
      "23000 [D loss: -0.291696] [G loss: 0.393167] [OOS loss: 0.058377] [Div loss: 0.817737]\n",
      "23500 [D loss: -0.246178] [G loss: 0.431494] [OOS loss: 0.077910] [Div loss: 0.855787]\n",
      "24000 [D loss: -0.367515] [G loss: 0.464406] [OOS loss: 0.159795] [Div loss: 0.756957]\n",
      "24500 [D loss: -0.376868] [G loss: 0.500373] [OOS loss: 0.181625] [Div loss: 0.786079]\n",
      "25000 [D loss: -0.276738] [G loss: 0.494115] [OOS loss: 0.204183] [Div loss: 0.727910]\n",
      "25500 [D loss: -0.288937] [G loss: 0.423951] [OOS loss: 0.136894] [Div loss: 0.722429]\n",
      "26000 [D loss: -0.385995] [G loss: 0.451660] [OOS loss: 0.124331] [Div loss: 0.802973]\n",
      "26500 [D loss: -0.250284] [G loss: 0.500665] [OOS loss: 0.185419] [Div loss: 0.778418]\n",
      "27000 [D loss: -0.324555] [G loss: 0.502667] [OOS loss: 0.184146] [Div loss: 0.785890]\n",
      "27500 [D loss: -0.190956] [G loss: 0.408429] [OOS loss: 0.133096] [Div loss: 0.699354]\n",
      "28000 [D loss: -0.318089] [G loss: 0.461484] [OOS loss: 0.190516] [Div loss: 0.690733]\n",
      "28500 [D loss: -0.316002] [G loss: 0.466622] [OOS loss: 0.126097] [Div loss: 0.828946]\n",
      "29000 [D loss: -0.345925] [G loss: 0.489545] [OOS loss: 0.139829] [Div loss: 0.847345]\n",
      "time : 4151.983514785767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\euijin\\.conda\\envs\\DGM\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 2.946622] [G loss: 4.806742] [OOS loss: 0.093409] [Div loss: 6.348338]\n",
      "500 [D loss: -1.199839] [G loss: 1.201620] [OOS loss: 0.361425] [Div loss: 1.190309]\n",
      "1000 [D loss: -1.051704] [G loss: 1.127373] [OOS loss: 0.557417] [Div loss: 0.832100]\n",
      "1500 [D loss: -0.893421] [G loss: 1.025580] [OOS loss: 0.488384] [Div loss: 0.789358]\n",
      "2000 [D loss: -0.802430] [G loss: 1.224481] [OOS loss: 0.700763] [Div loss: 0.771875]\n",
      "2500 [D loss: -0.706849] [G loss: 1.061853] [OOS loss: 0.553000] [Div loss: 0.752186]\n",
      "3000 [D loss: -0.769564] [G loss: 1.072818] [OOS loss: 0.539888] [Div loss: 0.785358]\n",
      "3500 [D loss: -0.619707] [G loss: 1.006907] [OOS loss: 0.534437] [Div loss: 0.705391]\n",
      "4000 [D loss: -0.725716] [G loss: 0.863187] [OOS loss: 0.427206] [Div loss: 0.656581]\n",
      "4500 [D loss: -0.612588] [G loss: 0.937299] [OOS loss: 0.503779] [Div loss: 0.653595]\n",
      "5000 [D loss: -0.586758] [G loss: 0.959576] [OOS loss: 0.487236] [Div loss: 0.705259]\n",
      "5500 [D loss: -0.574939] [G loss: 0.924026] [OOS loss: 0.453454] [Div loss: 0.703994]\n",
      "6000 [D loss: -0.625168] [G loss: 0.916943] [OOS loss: 0.464698] [Div loss: 0.679470]\n",
      "6500 [D loss: -0.654188] [G loss: 0.880860] [OOS loss: 0.507928] [Div loss: 0.574082]\n",
      "7000 [D loss: -0.596171] [G loss: 0.951967] [OOS loss: 0.524810] [Div loss: 0.645948]\n",
      "7500 [D loss: -0.500690] [G loss: 0.975749] [OOS loss: 0.495539] [Div loss: 0.717103]\n",
      "8000 [D loss: -0.498875] [G loss: 0.894300] [OOS loss: 0.457460] [Div loss: 0.659078]\n",
      "8500 [D loss: -0.518206] [G loss: 0.870451] [OOS loss: 0.448246] [Div loss: 0.639889]\n",
      "9000 [D loss: -0.503656] [G loss: 0.899552] [OOS loss: 0.478337] [Div loss: 0.638889]\n",
      "9500 [D loss: -0.625419] [G loss: 0.902274] [OOS loss: 0.512117] [Div loss: 0.597589]\n",
      "10000 [D loss: -0.486724] [G loss: 0.827976] [OOS loss: 0.399813] [Div loss: 0.648020]\n",
      "10500 [D loss: -0.450958] [G loss: 0.756811] [OOS loss: 0.425145] [Div loss: 0.519678]\n",
      "11000 [D loss: -0.562566] [G loss: 0.801864] [OOS loss: 0.374955] [Div loss: 0.646769]\n",
      "11500 [D loss: -0.452688] [G loss: 0.757713] [OOS loss: 0.307763] [Div loss: 0.677170]\n",
      "12000 [D loss: -0.416924] [G loss: 0.880800] [OOS loss: 0.405816] [Div loss: 0.710865]\n",
      "12500 [D loss: -0.524436] [G loss: 0.795964] [OOS loss: 0.393265] [Div loss: 0.614562]\n",
      "13000 [D loss: -0.493765] [G loss: 0.699670] [OOS loss: 0.344618] [Div loss: 0.551158]\n",
      "13500 [D loss: -0.479297] [G loss: 0.656628] [OOS loss: 0.220824] [Div loss: 0.658802]\n",
      "14000 [D loss: -0.502989] [G loss: 0.757377] [OOS loss: 0.330595] [Div loss: 0.646095]\n",
      "14500 [D loss: -0.487885] [G loss: 0.690205] [OOS loss: 0.323653] [Div loss: 0.566598]\n",
      "15000 [D loss: -0.458943] [G loss: 0.625586] [OOS loss: 0.255293] [Div loss: 0.571741]\n",
      "15500 [D loss: -0.550919] [G loss: 0.666524] [OOS loss: 0.257838] [Div loss: 0.622874]\n",
      "16000 [D loss: -0.442494] [G loss: 0.671478] [OOS loss: 0.252946] [Div loss: 0.636120]\n",
      "16500 [D loss: -0.402856] [G loss: 0.636243] [OOS loss: 0.220708] [Div loss: 0.632034]\n",
      "17000 [D loss: -0.421975] [G loss: 0.748123] [OOS loss: 0.325125] [Div loss: 0.641808]\n",
      "17500 [D loss: -0.487945] [G loss: 0.755783] [OOS loss: 0.375259] [Div loss: 0.585430]\n",
      "18000 [D loss: -0.539152] [G loss: 0.751390] [OOS loss: 0.362254] [Div loss: 0.596843]\n",
      "18500 [D loss: -0.468224] [G loss: 0.640696] [OOS loss: 0.215699] [Div loss: 0.644545]\n",
      "19000 [D loss: -0.473037] [G loss: 0.719716] [OOS loss: 0.345046] [Div loss: 0.577402]\n",
      "19500 [D loss: -0.570976] [G loss: 0.720494] [OOS loss: 0.279911] [Div loss: 0.665543]\n",
      "20000 [D loss: -0.408146] [G loss: 0.631247] [OOS loss: 0.183316] [Div loss: 0.675279]\n",
      "20500 [D loss: -0.506508] [G loss: 0.615391] [OOS loss: 0.187335] [Div loss: 0.648434]\n",
      "21000 [D loss: -0.437668] [G loss: 0.630471] [OOS loss: 0.170755] [Div loss: 0.690746]\n",
      "21500 [D loss: -0.485631] [G loss: 0.688890] [OOS loss: 0.304628] [Div loss: 0.590768]\n",
      "22000 [D loss: -0.561500] [G loss: 0.614127] [OOS loss: 0.243362] [Div loss: 0.573111]\n",
      "22500 [D loss: -0.478785] [G loss: 0.654795] [OOS loss: 0.181028] [Div loss: 0.710212]\n",
      "23000 [D loss: -0.490572] [G loss: 0.638876] [OOS loss: 0.212207] [Div loss: 0.647318]\n",
      "23500 [D loss: -0.465323] [G loss: 0.562514] [OOS loss: 0.166873] [Div loss: 0.605798]\n",
      "24000 [D loss: -0.367620] [G loss: 0.513320] [OOS loss: 0.141782] [Div loss: 0.573952]\n",
      "24500 [D loss: -0.442123] [G loss: 0.556040] [OOS loss: 0.216284] [Div loss: 0.531210]\n",
      "25000 [D loss: -0.393325] [G loss: 0.640913] [OOS loss: 0.227551] [Div loss: 0.629536]\n",
      "25500 [D loss: -0.369162] [G loss: 0.534411] [OOS loss: 0.150398] [Div loss: 0.590849]\n",
      "26000 [D loss: -0.438894] [G loss: 0.549540] [OOS loss: 0.133335] [Div loss: 0.633533]\n",
      "26500 [D loss: -0.444688] [G loss: 0.502916] [OOS loss: 0.128054] [Div loss: 0.578075]\n",
      "27000 [D loss: -0.438860] [G loss: 0.515918] [OOS loss: 0.145534] [Div loss: 0.572112]\n",
      "27500 [D loss: -0.364391] [G loss: 0.514285] [OOS loss: 0.132276] [Div loss: 0.587748]\n",
      "28000 [D loss: -0.422452] [G loss: 0.507500] [OOS loss: 0.127044] [Div loss: 0.585851]\n",
      "28500 [D loss: -0.400190] [G loss: 0.638066] [OOS loss: 0.204214] [Div loss: 0.657012]\n",
      "29000 [D loss: -0.416846] [G loss: 0.592655] [OOS loss: 0.193183] [Div loss: 0.611062]\n",
      "time : 6354.943909168243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\euijin\\.conda\\envs\\DGM\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 3.622234] [G loss: 4.717385] [OOS loss: -0.001377] [Div loss: 6.372538]\n",
      "500 [D loss: -1.114619] [G loss: 0.901132] [OOS loss: -0.008108] [Div loss: 1.300929]\n",
      "1000 [D loss: -0.941767] [G loss: 0.983563] [OOS loss: 0.370092] [Div loss: 0.907670]\n",
      "1500 [D loss: -0.826401] [G loss: 1.023832] [OOS loss: 0.478131] [Div loss: 0.818905]\n",
      "2000 [D loss: -0.845680] [G loss: 1.118835] [OOS loss: 0.600885] [Div loss: 0.782559]\n",
      "2500 [D loss: -0.732065] [G loss: 1.094143] [OOS loss: 0.615484] [Div loss: 0.731789]\n",
      "3000 [D loss: -0.801053] [G loss: 1.180424] [OOS loss: 0.705166] [Div loss: 0.727015]\n",
      "3500 [D loss: -0.680214] [G loss: 1.060416] [OOS loss: 0.646449] [Div loss: 0.646586]\n",
      "4000 [D loss: -0.581393] [G loss: 1.148587] [OOS loss: 0.679803] [Div loss: 0.719464]\n",
      "4500 [D loss: -0.701635] [G loss: 1.011102] [OOS loss: 0.639688] [Div loss: 0.590493]\n",
      "5000 [D loss: -0.688979] [G loss: 1.045122] [OOS loss: 0.632597] [Div loss: 0.645104]\n",
      "5500 [D loss: -0.650278] [G loss: 1.061838] [OOS loss: 0.688485] [Div loss: 0.593323]\n",
      "6000 [D loss: -0.670698] [G loss: 1.076726] [OOS loss: 0.665916] [Div loss: 0.643264]\n",
      "6500 [D loss: -0.724766] [G loss: 0.952372] [OOS loss: 0.584869] [Div loss: 0.585846]\n",
      "7000 [D loss: -0.650160] [G loss: 0.984602] [OOS loss: 0.567117] [Div loss: 0.652602]\n",
      "7500 [D loss: -0.529926] [G loss: 0.930517] [OOS loss: 0.505527] [Div loss: 0.662204]\n",
      "8000 [D loss: -0.524270] [G loss: 0.895327] [OOS loss: 0.498633] [Div loss: 0.624374]\n",
      "8500 [D loss: -0.567474] [G loss: 0.941259] [OOS loss: 0.520149] [Div loss: 0.657918]\n",
      "9000 [D loss: -0.594982] [G loss: 0.787312] [OOS loss: 0.437986] [Div loss: 0.561783]\n",
      "9500 [D loss: -0.664594] [G loss: 0.831137] [OOS loss: 0.456190] [Div loss: 0.596921]\n",
      "10000 [D loss: -0.653222] [G loss: 0.829668] [OOS loss: 0.422401] [Div loss: 0.638715]\n",
      "10500 [D loss: -0.466276] [G loss: 0.735993] [OOS loss: 0.339708] [Div loss: 0.624752]\n",
      "11000 [D loss: -0.568117] [G loss: 0.649104] [OOS loss: 0.262242] [Div loss: 0.611837]\n",
      "11500 [D loss: -0.612199] [G loss: 0.713447] [OOS loss: 0.320627] [Div loss: 0.621022]\n",
      "12000 [D loss: -0.608087] [G loss: 0.559072] [OOS loss: 0.221372] [Div loss: 0.547437]\n",
      "12500 [D loss: -0.449948] [G loss: 0.661342] [OOS loss: 0.266262] [Div loss: 0.624064]\n",
      "13000 [D loss: -0.629445] [G loss: 0.591312] [OOS loss: 0.205035] [Div loss: 0.612044]\n",
      "13500 [D loss: -0.485585] [G loss: 0.593659] [OOS loss: 0.169455] [Div loss: 0.662624]\n",
      "14000 [D loss: -0.494268] [G loss: 0.542399] [OOS loss: 0.097877] [Div loss: 0.690006]\n",
      "14500 [D loss: -0.515419] [G loss: 0.506241] [OOS loss: 0.134370] [Div loss: 0.592536]\n",
      "15000 [D loss: -0.585772] [G loss: 0.410958] [OOS loss: 0.032487] [Div loss: 0.601465]\n",
      "15500 [D loss: -0.556069] [G loss: 0.368127] [OOS loss: 0.056064] [Div loss: 0.513899]\n",
      "16000 [D loss: -0.487283] [G loss: 0.388935] [OOS loss: 0.003137] [Div loss: 0.611711]\n",
      "16500 [D loss: -0.482397] [G loss: 0.347538] [OOS loss: -0.018280] [Div loss: 0.584985]\n",
      "17000 [D loss: -0.507835] [G loss: 0.465330] [OOS loss: 0.079542] [Div loss: 0.611877]\n",
      "17500 [D loss: -0.473272] [G loss: 0.399139] [OOS loss: 0.036480] [Div loss: 0.580754]\n",
      "18000 [D loss: -0.479025] [G loss: 0.335388] [OOS loss: 0.008096] [Div loss: 0.533382]\n",
      "18500 [D loss: -0.558959] [G loss: 0.453036] [OOS loss: 0.026540] [Div loss: 0.665944]\n",
      "19000 [D loss: -0.499686] [G loss: 0.368533] [OOS loss: -0.001327] [Div loss: 0.590907]\n",
      "19500 [D loss: -0.519092] [G loss: 0.449919] [OOS loss: 0.042809] [Div loss: 0.640091]\n",
      "20000 [D loss: -0.490391] [G loss: 0.404575] [OOS loss: -0.009979] [Div loss: 0.650872]\n",
      "20500 [D loss: -0.452508] [G loss: 0.399814] [OOS loss: -0.019709] [Div loss: 0.657288]\n",
      "21000 [D loss: -0.443674] [G loss: 0.353546] [OOS loss: -0.085228] [Div loss: 0.682565]\n",
      "21500 [D loss: -0.459812] [G loss: 0.400056] [OOS loss: -0.001451] [Div loss: 0.632545]\n",
      "22000 [D loss: -0.501787] [G loss: 0.403285] [OOS loss: 0.027588] [Div loss: 0.599205]\n",
      "22500 [D loss: -0.564887] [G loss: 0.411125] [OOS loss: -0.017969] [Div loss: 0.669868]\n",
      "23000 [D loss: -0.364853] [G loss: 0.348438] [OOS loss: -0.060221] [Div loss: 0.642357]\n",
      "23500 [D loss: -0.510992] [G loss: 0.326982] [OOS loss: -0.022958] [Div loss: 0.564260]\n",
      "24000 [D loss: -0.443580] [G loss: 0.247764] [OOS loss: -0.158867] [Div loss: 0.640565]\n",
      "24500 [D loss: -0.474555] [G loss: 0.278842] [OOS loss: -0.112194] [Div loss: 0.619484]\n",
      "25000 [D loss: -0.491709] [G loss: 0.351702] [OOS loss: -0.078489] [Div loss: 0.671375]\n",
      "25500 [D loss: -0.489769] [G loss: 0.355194] [OOS loss: 0.023789] [Div loss: 0.540076]\n",
      "26000 [D loss: -0.486448] [G loss: 0.357108] [OOS loss: -0.073014] [Div loss: 0.671533]\n",
      "26500 [D loss: -0.431545] [G loss: 0.375221] [OOS loss: -0.081227] [Div loss: 0.706710]\n",
      "27000 [D loss: -0.557474] [G loss: 0.306492] [OOS loss: -0.073799] [Div loss: 0.605320]\n",
      "27500 [D loss: -0.472584] [G loss: 0.284591] [OOS loss: -0.048074] [Div loss: 0.541282]\n",
      "28000 [D loss: -0.493645] [G loss: 0.286025] [OOS loss: -0.004824] [Div loss: 0.485927]\n",
      "28500 [D loss: -0.397688] [G loss: 0.304392] [OOS loss: -0.063042] [Div loss: 0.587773]\n",
      "29000 [D loss: -0.572734] [G loss: 0.291669] [OOS loss: -0.059021] [Div loss: 0.566141]\n",
      "time : 8555.0152592659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\euijin\\.conda\\envs\\DGM\\lib\\site-packages\\keras\\engine\\training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from random import sample\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from itertools import combinations\n",
    "\n",
    "## Hyperparameter calibration\n",
    "beta1_grid = [0.5,0.75]\n",
    "beta2_grid = [0.004,0.005]\n",
    "Performance_measures = []\n",
    "\n",
    "param_grid = []\n",
    "for r in itertools.product(beta1_grid, beta2_grid):\n",
    "    param_grid.append([r[0],r[1]])\n",
    "\n",
    "# Load the dataset\n",
    "X_train = pd.get_dummies(train_test_split(x_population,test_size=0.05,shuffle=True,random_state=1004)[1]).values.astype(\"float32\")\n",
    "\n",
    "for beta in param_grid:\n",
    "    \n",
    "    # # Load the dataset\n",
    "    # X_train = pd.get_dummies(train_test_split(x_population,test_size=0.05,shuffle=True,random_state=None)[1]).values.astype(\"float32\")\n",
    "\n",
    "    ## Setting hyperparameters from MultiCATGAN\n",
    "    intermediate_dim = [256,256,256]\n",
    "    latent_dim = 128\n",
    "    optimizer = Adam(\n",
    "        learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    "    )\n",
    "    BATCH_SIZE = 256\n",
    "    beta1=beta[0]\n",
    "    beta2=beta[1]\n",
    "\n",
    "    ## Model Build\n",
    "    generator = build_generator()\n",
    "    critic = build_critic()\n",
    "\n",
    "    #-------------------------------\n",
    "    # Construct Computational Graph\n",
    "    #       for the Critic\n",
    "    #-------------------------------\n",
    "\n",
    "    # Freeze generator's layers while training critic\n",
    "    generator.trainable = False\n",
    "\n",
    "\n",
    "    # Image input (real sample)\n",
    "    real_img = Input(shape=x_train.shape[1])\n",
    "\n",
    "    # Noise input\n",
    "    z_disc = Input(shape=(latent_dim))\n",
    "    # Generate image based of noise (fake sample) and add label to the input \n",
    "    fake_img = generator(z_disc)\n",
    "\n",
    "    # Discriminator determines validity of the real and fake images\n",
    "    fake = critic(fake_img)\n",
    "    valid = critic(real_img)\n",
    "\n",
    "\n",
    "    # Construct weighted average between real and fake images\n",
    "    interpolated_img = RandomWeightedAverage([real_img, fake_img])\n",
    "\n",
    "    # Determine validity of weighted sample\n",
    "    validity_interpolated = critic(interpolated_img)\n",
    "\n",
    "    partial_gp_loss = partial(gradient_penalty_loss,averaged_samples=interpolated_img)\n",
    "    partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "    critic_model = Model(inputs=[real_img,z_disc], outputs=[valid, fake, validity_interpolated])\n",
    "    critic_model.compile(loss=[wasserstein_loss,\n",
    "                               wasserstein_loss,\n",
    "                               partial_gp_loss],\n",
    "                               optimizer=optimizer,\n",
    "                               loss_weights=[1, 1, 10])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #-------------------------------\n",
    "    # Construct Computational Graph\n",
    "    #         for Generator\n",
    "    #-------------------------------\n",
    "\n",
    "    # For the generator we freeze the critic's layers\n",
    "    critic.trainable = False\n",
    "    generator.trainable = True\n",
    "\n",
    "    # Sampled noise for input to generator\n",
    "    z_gen = Input(shape=(latent_dim))\n",
    "\n",
    "    # Generate images based of noise\n",
    "    img = generator(z_gen)\n",
    "\n",
    "    # Discriminator determines validity\n",
    "    valid = critic(img)\n",
    "\n",
    "    # Defines generator model\n",
    "    generator_model = Model(z_gen, [valid,img,img])\n",
    "    generator_model.compile(loss=[wasserstein_loss,OOS_loss,div_loss], optimizer=optimizer,\n",
    "                            loss_weights=[1,beta1,beta2])\n",
    "\n",
    "    # Adversarial ground truths\n",
    "    valid = - np.ones(BATCH_SIZE)\n",
    "    fake =  np.ones(BATCH_SIZE)\n",
    "    dummy = np.zeros(BATCH_SIZE) # Dummy gt for gradient penalty\n",
    "    for epoch in range(epochs):\n",
    "        for _ in range(n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            imgs = X_train[np.random.randint(0,X_train.shape[0],BATCH_SIZE)]\n",
    "\n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0,1,[BATCH_SIZE,latent_dim]) \n",
    "\n",
    "            # Train the critic\n",
    "            d_loss = critic_model.train_on_batch([imgs, noise], [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        g_loss = generator_model.train_on_batch(noise, [valid,valid,valid])\n",
    "\n",
    "\n",
    "        # If at save interval => save generated image samples\n",
    "        if epoch % sample_interval == 0:\n",
    "            print (\"%d [D loss: %f] [G loss: %f] [OOS loss: %f] [Div loss: %f]\" % (epoch, d_loss[0], g_loss[0], g_loss[1], g_loss[2]))\n",
    "            losslog.append([d_loss[0], g_loss])\n",
    "            generator.save_weights('C:/Users/euijin/Documents/modelCollection/WGAN/WGAN_OOSdivH'+str(beta), overwrite=True)\n",
    "            #critic.save_weights('WGAN_Eager_critic/WGAN_Naive_Eager_F1', overwrite=True)\n",
    "    \n",
    "    print(\"time :\", time.time() - start) \n",
    "    \n",
    "    def generate_images(size=x_sample.shape[0]):\n",
    "        generator.load_weights('C:/Users/euijin/Documents/modelCollection/WGAN/WGAN_OOSdivH'+str(beta))\n",
    "        noise = np.random.normal(0, 1, (size,latent_dim))\n",
    "        gen_imgs = generator.predict(noise)\n",
    "   \n",
    "        return gen_imgs\n",
    "\n",
    "\n",
    "    ## Make Ground Truth & Test\n",
    "    n_uni_col = [x_sample[i].nunique() for i in x_sample.columns]\n",
    "    n_uni_col = [0]+n_uni_col\n",
    "    n_uni_col = np.cumsum(n_uni_col)\n",
    "\n",
    "    col_pop = x_train.columns.values.copy()\n",
    "    for i in range(len(col_pop)):\n",
    "        col_pop[i] = col_pop[i].split('_', 1)[1]\n",
    "    col_pop = col_pop.astype('int64')\n",
    "    \n",
    "    ## Generate samples\n",
    "    gen_sample = generate_images(x_sample.shape[0])\n",
    "    resamples = wide_to_long(gen_sample)\n",
    "    resamples = resamples.astype('int64')\n",
    "    resamples = resamples.astype('category')\n",
    "    \n",
    "    ## Distributional distance\n",
    "    Marg_JSD,Bi_JSD = mean_JSD(x_population,resamples)\n",
    "    Marg_SRMSE,Bi_SRMSE = SRMSE(x_population,resamples)\n",
    "    \n",
    "    \n",
    "    ## Extract the STZ/SAZ, Precision/Recall\n",
    "    p_pop = x_population.iloc[:,0].astype(str)\n",
    "    for i in range(x_population.shape[1]-1):\n",
    "        p_pop = p_pop+x_population.iloc[:,(i+1)].astype(str)\n",
    "\n",
    "    p_gen = resamples.astype('int64').iloc[:,0].astype(str)\n",
    "    for i in range(resamples.astype('int64').shape[1]-1):\n",
    "        p_gen = p_gen+resamples.astype('int64').iloc[:,(i+1)].astype(str)\n",
    "\n",
    "    p_sam = x_sample.iloc[:,0].astype(str)\n",
    "    for i in range(x_sample.shape[1]-1):\n",
    "        p_sam = p_sam+x_sample.iloc[:,(i+1)].astype(str)\n",
    "        \n",
    "    STZ = resamples[~p_gen.isin(p_sam) & ~p_gen.isin(p_pop)]\n",
    "    SAZ = resamples[~p_gen.isin(p_sam) & p_gen.isin(p_pop)]\n",
    "    idx_STZ = ~p_gen.isin(p_sam) & ~p_gen.isin(p_pop)\n",
    "    idx_SAZ = ~p_gen.isin(p_sam) & p_gen.isin(p_pop)\n",
    "    STZ_ratio = round(STZ.shape[0]/p_gen.shape[0],3)\n",
    "    SAZ_ratio = round(SAZ.shape[0]/p_gen.shape[0],3)\n",
    "    Precision = round(np.mean(p_gen.isin(p_pop)),3)\n",
    "    Recall = round(np.mean(p_pop.isin(p_gen)),3)\n",
    "    Num_comb = p_gen.nunique()\n",
    "    \n",
    "    Performance_measures.append([round(Marg_SRMSE,3),round(Bi_SRMSE,3),Num_comb,Precision,Recall])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-07T08:21:21.442196Z",
     "iopub.status.busy": "2022-07-07T08:21:21.442196Z",
     "iopub.status.idle": "2022-07-07T08:21:21.458143Z",
     "shell.execute_reply": "2022-07-07T08:21:21.458143Z",
     "shell.execute_reply.started": "2022-07-07T08:21:21.442196Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Outputs = pd.concat([pd.DataFrame(param_grid),pd.DataFrame(np.concatenate(Performance_measures).reshape(-1,5))],axis=1)\n",
    "Outputs.to_clipboard()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "## Make Ground Truth & Test\n",
    "n_uni_col = [x_sample[i].nunique() for i in x_sample.columns]\n",
    "n_uni_col = [0]+n_uni_col\n",
    "n_uni_col = np.cumsum(n_uni_col)\n",
    "\n",
    "col_pop = x_train.columns.values.copy()\n",
    "for i in range(len(col_pop)):b\n",
    "    col_pop[i] = col_pop[i].split('_', 1)[1]\n",
    "col_pop = col_pop.astype('int64')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gen_sample = generate_images(x_sample.shape[0])\n",
    "resamples = wide_to_long(gen_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "Marg_JSD,Bi_JSD = mean_JSD(x_sample,resamples)\n",
    "print(np.mean(Marg_JSD),np.mean(Bi_JSD))\n",
    "Marg_JSD,Bi_JSD = mean_JSD(x_population,resamples)\n",
    "print(np.mean(Marg_JSD),np.mean(Bi_JSD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Extract the STZ and SAZ\n",
    "p_pop = x_population.iloc[:,0].astype(str)\n",
    "for i in range(x_population.shape[1]-1):\n",
    "    p_pop = p_pop+x_population.iloc[:,(i+1)].astype(str)\n",
    "    \n",
    "p_gen = resamples.astype('int64').iloc[:,0].astype(str)\n",
    "for i in range(resamples.astype('int64').shape[1]-1):\n",
    "    p_gen = p_gen+resamples.astype('int64').iloc[:,(i+1)].astype(str)\n",
    "    \n",
    "p_sam = x_sample.iloc[:,0].astype(str)\n",
    "for i in range(x_sample.shape[1]-1):\n",
    "    p_sam = p_sam+x_sample.iloc[:,(i+1)].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print({'out-of-popgen': round(1-np.sum(p_gen.isin(p_pop))/len(p_gen),3),\n",
    "       'out-of-samgen': round(1-np.sum(p_gen.isin(p_sam))/len(p_gen),3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STZ = resamples[~p_gen.isin(p_sam) & ~p_gen.isin(p_pop)]\n",
    "SAZ = resamples[~p_gen.isin(p_sam) & p_gen.isin(p_pop)]\n",
    "idx_STZ = ~p_gen.isin(p_sam) & ~p_gen.isin(p_pop)\n",
    "idx_SAZ = ~p_gen.isin(p_sam) & p_gen.isin(p_pop)\n",
    "print(round(STZ.shape[0]/p_gen.shape[0],3),round(SAZ.shape[0]/p_gen.shape[0],3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(np.mean(p_pop.isin(p_sam)))\n",
    "print(np.mean(p_pop.isin(p_gen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Precision = np.mean(p_gen.isin(p_pop))\n",
    "Recall = np.mean(p_pop.isin(p_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Precision = []\n",
    "Recall = []\n",
    "for num in range(430000,1130000,100000):\n",
    "    t_gen_sample = generate_images(num)\n",
    "    t_resamples = wide_to_long(t_gen_sample)\n",
    "    t_p_gen = t_resamples.astype('int64').iloc[:,0].astype(str)\n",
    "    for i in range(t_resamples.astype('int64').shape[1]-1):\n",
    "        t_p_gen = t_p_gen+t_resamples.astype('int64').iloc[:,(i+1)].astype(str)\n",
    "    Precision.append(np.mean(t_p_gen.isin(p_pop)))\n",
    "    Recall.append(np.mean(p_pop.isin(t_p_gen)))\n",
    "    print(num); print(Precision); print(Recall)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-22T13:46:10.713443Z",
     "iopub.status.busy": "2022-06-22T13:46:10.713443Z",
     "iopub.status.idle": "2022-06-22T13:46:10.741338Z",
     "shell.execute_reply": "2022-06-22T13:46:10.740368Z",
     "shell.execute_reply.started": "2022-06-22T13:46:10.713443Z"
    },
    "tags": []
   },
   "source": [
    "pd.DataFrame(np.concatenate([Precision,Recall],axis=0).reshape(2,4)).to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(x_population)\n",
    "\n",
    "x_pop_dum = enc.transform(x_population).toarray()\n",
    "x_gen_dum = enc.transform(resamples).toarray()\n",
    "x_sam_dum = enc.transform(x_sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Precision & Recall\n",
    "import numpy as np\n",
    "from prdc import compute_prdc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import BallTree \n",
    "import sklearn.metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_nearest_neighbour_distances(input_features, nearest_k):\n",
    "\n",
    "    tree = BallTree(input_features,leaf_size = 40,metric='l1')\n",
    "    dist, _ = tree.query(input_features, k=(nearest_k+1)) \n",
    "    radii = dist\n",
    "    return radii\n",
    "\n",
    "def compute_pairwise_distance(data_x, data_y=None):\n",
    "\n",
    "    if data_y is None:\n",
    "        data_y = data_x\n",
    "    dists = sklearn.metrics.pairwise_distances(\n",
    "        data_x, data_y, metric='l2', n_jobs=-1)\n",
    "    return dists\n",
    "\n",
    "def compute_pairwise_hamming_distance(data_x, data_y=None):\n",
    "\n",
    "    if data_y is None:\n",
    "        data_y = data_x\n",
    "    dists = sklearn.metrics.pairwise_distances(\n",
    "        data_x, data_y, metric='hamming', n_jobs=-1)\n",
    "    return dists\n",
    "\n",
    "def compute_pairwise_nearest_distance(data_x, data_y):\n",
    "\n",
    "    kd_tree1 = cKDTree(data_x)\n",
    "    kd_tree2 = cKDTree(data_y)\n",
    "    r = np.max(compute_nearest_neighbour_distances(\n",
    "    data_x, nearest_k))\n",
    "    indexes = kd_tree1.query_ball_tree(kd_tree2, r=0.2,p=1)\n",
    "    \n",
    "    \n",
    "    dists = sklearn.metrics.pairwise_distances(\n",
    "        data_x, data_y, metric='l1', n_jobs=-1)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "mlm_model = tf.keras.models.load_model(\n",
    "    \"MLM_Embed_Indiv5_New.h5\")\n",
    "# mlm_model = tf.keras.models.load_model(\n",
    "#     \"MLM_Embed_Indiv5.h5\")\n",
    "\n",
    "embedding_layers =  mlm_model.layers[13:26] ## Find a embedding layers from mlm_model.summary()\n",
    "\n",
    "def convert_to_embedding(samples):\n",
    "    samples_emb = []\n",
    "    for i in range(len(embedding_layers)):\n",
    "        emb_weight = embedding_layers[i].get_weights()[0]\n",
    "        trgt = samples[:,range(n_uni_col[i],n_uni_col[i+1])]\n",
    "        samples_emb.append(np.dot(trgt,emb_weight))\n",
    "    \n",
    "    return(np.concatenate(samples_emb,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sam_embed = convert_to_embedding(x_sam_dum)\n",
    "#nearest_k = 3\n",
    "#real_nearest_neighbour_distances = compute_nearest_neighbour_distances(sam_embed, nearest_k) ## 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STZ_dum = enc.transform(STZ).toarray()\n",
    "STZ_embed = convert_to_embedding(STZ_dum)\n",
    "SAZ_dum = enc.transform(SAZ).toarray()\n",
    "SAZ_embed = convert_to_embedding(SAZ_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "STZ_distance = compute_pairwise_distance(STZ_embed,sam_embed)\n",
    "STZ_nearest_distance = np.min(STZ_distance,axis=1)\n",
    "\n",
    "SAZ_distance = compute_pairwise_distance(SAZ_embed,sam_embed)\n",
    "SAZ_nearest_distance = np.min(SAZ_distance,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_histogram(x,y):\n",
    "    # q25, q75 = np.percentile(y, [25, 75])\n",
    "    # bin_width = 2 * (q75 - q25) * len(x) ** (-1/3)\n",
    "    # bins = round((x.max() - x.min()) / bin_width)\n",
    "    bins = 50\n",
    "    \n",
    "    plt.hist(x,bins=bins,density=True,label='STZ',alpha=0.5)\n",
    "    plt.hist(y,bins=bins,density=True,label='SAZ',alpha=0.5)\n",
    "    \n",
    "    mn, mx = plt.xlim()\n",
    "    plt.xlim(mn, mx)\n",
    "    kde_xs = np.linspace(mn, mx, 300)\n",
    "    kde = st.gaussian_kde(x)\n",
    "    plt.plot(kde_xs, kde.pdf(kde_xs), label=\"STZ_PDF\")\n",
    "\n",
    "    mn, mx = plt.xlim()\n",
    "    plt.xlim(mn, mx)\n",
    "    kde_xs = np.linspace(mn, mx, 300)\n",
    "    kde = st.gaussian_kde(y)\n",
    "    plt.plot(kde_xs, kde.pdf(kde_xs), label=\"SAZ_PDF\")\n",
    "    \n",
    "\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.xlabel(\"Nearest distance from sample\")\n",
    "    print(\"Freedman–Diaconis number of bins:\", bins)\n",
    "    plt.show()\n",
    "\n",
    "plot_histogram(STZ_nearest_distance,SAZ_nearest_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Compute hamming distance of STZ and SAZ\n",
    "from scipy.spatial.distance import hamming\n",
    "\n",
    "STZ_hamming_distance = compute_pairwise_hamming_distance(STZ_dum,x_sam_dum)\n",
    "SAZ_hamming_distance = compute_pairwise_hamming_distance(SAZ_dum,x_sam_dum)\n",
    "STZ_hamming_distance  = np.min(STZ_hamming_distance ,axis=1)\n",
    "SAZ_hamming_distance  = np.min(SAZ_hamming_distance ,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Construct logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "data_distance_to_manifold = pd.DataFrame({'DTM': np.hstack([STZ_nearest_distance,SAZ_nearest_distance]),\n",
    "                                          'label': np.hstack([np.repeat(1,len(STZ_nearest_distance)),np.repeat(2,len(SAZ_nearest_distance))])},\n",
    "                                         index =np.arange(0,(len(STZ_nearest_distance)+len(SAZ_nearest_distance))))\n",
    "\n",
    "X = np.array(data_distance_to_manifold.DTM).reshape(-1,1)\n",
    "y = np.array(data_distance_to_manifold.label)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Construct logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "data_distance_to_manifold = pd.DataFrame({'DTM': np.hstack([STZ_hamming_distance,SAZ_hamming_distance]),\n",
    "                                          'label': np.hstack([np.repeat(1,len(STZ_hamming_distance)),np.repeat(2,len(SAZ_hamming_distance))])},\n",
    "                                         index =np.arange(0,(len(STZ_hamming_distance)+len(SAZ_hamming_distance))))\n",
    "\n",
    "X = np.array(data_distance_to_manifold.DTM).reshape(-1,1)\n",
    "y = np.array(data_distance_to_manifold.label)\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "clf.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random selection\n",
    "(STZ.shape[0]/(STZ.shape[0]+SAZ.shape[0]))**2 + (SAZ.shape[0]/(STZ.shape[0]+SAZ.shape[0]))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_prdc(real_features_sam,fake_features_sam,nearest_k):\n",
    "    \n",
    "    print('Num real: {} Num fake: {}'\n",
    "      .format(real_features_sam.shape[0], fake_features_sam.shape[0]))\n",
    "    \n",
    "    real_nearest_neighbour_distances = compute_nearest_neighbour_distances(\n",
    "        real_features_sam, nearest_k)\n",
    "    fake_nearest_neighbour_distances = compute_nearest_neighbour_distances(\n",
    "        fake_features_sam, nearest_k)\n",
    "    distance_real_fake = compute_pairwise_distance(\n",
    "        real_features_sam, fake_features_sam)\n",
    "\n",
    "    precision = (\n",
    "            distance_real_fake <\n",
    "            np.expand_dims(real_nearest_neighbour_distances, axis=1)\n",
    "    ).any(axis=0).mean()\n",
    "\n",
    "    recall = (\n",
    "            distance_real_fake <\n",
    "            np.expand_dims(fake_nearest_neighbour_distances, axis=0)\n",
    "    ).any(axis=1).mean()\n",
    "\n",
    "    density = (1. / float(nearest_k)) * (\n",
    "            distance_real_fake <\n",
    "            np.expand_dims(real_nearest_neighbour_distances, axis=1)\n",
    "    ).sum(axis=0).mean()\n",
    "\n",
    "    coverage = (\n",
    "            distance_real_fake.min(axis=1) <\n",
    "            real_nearest_neighbour_distances\n",
    "    ).mean()\n",
    "\n",
    "\n",
    "    return np.array([round(precision,3),round(recall,3),round(density,3), round(coverage,3)])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T10:31:32.168424Z",
     "iopub.status.busy": "2022-06-20T10:31:32.167427Z",
     "iopub.status.idle": "2022-06-20T10:31:32.934404Z",
     "shell.execute_reply": "2022-06-20T10:31:32.934404Z",
     "shell.execute_reply.started": "2022-06-20T10:31:32.168424Z"
    },
    "tags": []
   },
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=4, ncols=3)\n",
    "ax = np.reshape(ax,(-1))\n",
    "colors = ['red', 'lime']\n",
    "label=['Generated', 'Training']\n",
    "\n",
    "i=0\n",
    "for i in range(len(x_sample.columns[:-1])):\n",
    "    \n",
    "    resam = pd.value_counts(resamples.iloc[:,i]).sort_index()\n",
    "    sam = pd.value_counts(x_sample.iloc[:,i]).sort_index()\n",
    "    tab = pd.merge(resam,sam,left_index=True, right_index=True,how='right')\n",
    "    tab = tab.fillna(0)\n",
    "    ticks = np.arange(tab.shape[0])\n",
    "    w = 0.3\n",
    "    ax[i].bar(ticks-0.5*w,tab.iloc[:,0],align='center',width =  w,color=colors[0], label=label[0])\n",
    "    ax[i].bar(ticks+0.5*w,tab.iloc[:,1],align='center',width =  w,color=colors[1], label=label[1])\n",
    "    ax[i].set_title(x_sample.columns[i],fontsize=8)\n",
    "    ax[i].tick_params(axis='y', labelsize=5)\n",
    "    #ax[i].autoscale(tight=True)\n",
    "    ax[i].set_xticks(ticks)\n",
    "    ax[i].set_xticklabels(labels = tab.index.to_list(),fontsize=5,rotation=40)\n",
    "\n",
    "plt.subplots_adjust(left=0.15, right=0.85, top=0.90,bottom=0.10,wspace = 0.2,hspace=0.2)\n",
    "plt.rcParams[\"figure.figsize\"] = (8,12)\n",
    "plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.savefig('line_plot_hq.png', dpi=300)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Compute two vector comparison\n",
    "# compare two vectors: Y_test (true) and Y_pred (predicted)\n",
    "from matplotlib.offsetbox import AnchoredText\n",
    "anchored_text = AnchoredText(\"Test\", loc=2)\n",
    "\n",
    "def compute_stat(Y_test, Y_pred, do_plot, plot_log):\n",
    "    Y_test, Y_pred = np.array(Y_test), np.array(Y_pred)\n",
    "    \n",
    "    corr_mat = np.corrcoef(Y_test, Y_pred)\n",
    "    corr = corr_mat[0, 1]\n",
    "    if np.isnan(corr): corr = 0.0\n",
    "    # MAE\n",
    "    mae = np.absolute(Y_test - Y_pred).mean()\n",
    "    # RMSE\n",
    "    rmse = np.linalg.norm(Y_test - Y_pred) / np.sqrt(len(Y_test))\n",
    "    # SRMSE\n",
    "    ybar = Y_test.mean()\n",
    "    srmse = rmse / ybar\n",
    "    # r-square\n",
    "    u = np.sum((Y_pred - Y_test)**2)\n",
    "    v = np.sum((Y_test - ybar)**2)\n",
    "    r2 = 1.0 - u / v\n",
    "    stat = {'mae': mae, 'rmse': rmse, 'r2': r2, 'srmse': srmse, 'corr': corr}\n",
    "    if do_plot:\n",
    "        min_Y = min([min(Y_test),min(Y_pred)])\n",
    "        max_Y = max([max(Y_test),max(Y_pred)])\n",
    "        w = max_Y - min_Y\n",
    "        max_Y += w * 0.02\n",
    "        text = ['SMRSE = {:.3f}'.format(stat['srmse']),\n",
    "                'Corr = {:.3f}'.format(stat['corr']),\n",
    "                '$R^2$ = {:.3f}'.format(stat['r2'])]\n",
    "        text = '\\n'.join(text)\n",
    "        plt.annotate(text , xy=(0.10, 0.75), xycoords='axes fraction')\n",
    "        plt.plot(Y_test, Y_pred, '.', alpha=0.5, ms=10, color='seagreen', markeredgewidth=0)\n",
    "        plt.plot([min_Y, max_Y], [min_Y, max_Y], ls='--', color='gray', linewidth=1.0)\n",
    "        plt.axis([min_Y, max_Y, min_Y, max_Y])\n",
    "        plt.xlabel('Obs')\n",
    "        plt.ylabel('Gen')\n",
    "        if plot_log:\n",
    "            eps = 1e-6\n",
    "            plt.axis([max(min_Y, eps), max_Y, max(min_Y, eps), max_Y])\n",
    "            plt.yscale('log')\n",
    "            plt.xscale('log')\n",
    "        #fig.savefig(os.path.join(temp_dir, '{}.pdf'.format(ylabel)), dpi=fig.dpi)\n",
    "        #plt.show()\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess\n",
    "\n",
    "\n",
    "## Aggregated Marginal distribution\n",
    "\n",
    "sam_marg_cnt = []\n",
    "resam_marg_cnt = []\n",
    "for col in x_population.columns:\n",
    "    resam = pd.value_counts(resamples[col]).sort_index()\n",
    "    sam = pd.value_counts(x_population[col]).sort_index()\n",
    "    tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "    tab = tab.fillna(0)\n",
    "    sam_marg_cnt.append(tab.iloc[:,1].values/x_population.shape[0])\n",
    "    resam_marg_cnt.append(tab.iloc[:,0].values/resamples.shape[0])\n",
    "\n",
    "sam_marg_cnt = np.concatenate(sam_marg_cnt)\n",
    "resam_marg_cnt = np.concatenate(resam_marg_cnt)\n",
    "\n",
    "  \n",
    "## Aggregated Bivariate distribution\n",
    "    \n",
    "bi_index = combinations(x_population.columns,2)\n",
    "bi_index = list(bi_index)\n",
    "col1,col2 = bi_index[0]\n",
    "\n",
    "sam_bi_cnt  = []\n",
    "resam_bi_cnt = []\n",
    "for col1,col2 in bi_index:\n",
    "    resam = pd.DataFrame(pd.crosstab(resamples[col1],resamples[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "    sam = pd.DataFrame(pd.crosstab(x_population[col1],x_population[col2],rownames=[col1],colnames=[col2]).stack().sort_index())\n",
    "    tab = pd.merge(resam,sam,left_index=True, right_index=True,how='outer')\n",
    "    tab = tab.fillna(0)\n",
    "    sam_bi_cnt.append(tab.iloc[:,1].values/x_population.shape[0])\n",
    "    resam_bi_cnt.append(tab.iloc[:,0].values/resamples.shape[0])\n",
    "\n",
    "sam_bi_cnt = np.concatenate(sam_bi_cnt)\n",
    "resam_bi_cnt = np.concatenate(resam_bi_cnt)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-20T05:05:23.893663Z",
     "iopub.status.busy": "2022-06-20T05:05:23.893663Z",
     "iopub.status.idle": "2022-06-20T05:05:24.053238Z",
     "shell.execute_reply": "2022-06-20T05:05:24.052239Z",
     "shell.execute_reply.started": "2022-06-20T05:05:23.893663Z"
    },
    "tags": []
   },
   "source": [
    "plt.subplot(2,1,1)\n",
    "compute_stat(sam_marg_cnt,resam_marg_cnt,True,False)\n",
    "plt.subplot(2,1,2)\n",
    "compute_stat(sam_bi_cnt,resam_bi_cnt,True,False)\n",
    "\n",
    "#plt.subplots_adjust(left=0.15, right=0.85, top=0.90,bottom=0.10,wspace = 0.2,hspace=0.2)\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "plt.rcParams[\"legend.loc\"] = 'upper right'\n",
    "#fig.tight_layout()\n",
    "plt.savefig('Fig_distribution_2021.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pretrained bert model\n",
    "mlm_model = tf.keras.models.load_model(\n",
    "    \"MLM_Embed_Indiv5_New.h5\")\n",
    "# mlm_model = tf.keras.models.load_model(\n",
    "#     \"MLM_Embed_Indiv5.h5\")\n",
    "\n",
    "embedding_layers =  mlm_model.layers[13:26] ## Find a embedding layers from mlm_model.summary()\n",
    "\n",
    "def convert_to_embedding(samples):\n",
    "    samples_emb = []\n",
    "    for i in range(len(embedding_layers)):\n",
    "        emb_weight = embedding_layers[i].get_weights()[0]\n",
    "        trgt = samples[:,range(n_uni_col[i],n_uni_col[i+1])]\n",
    "        samples_emb.append(np.dot(trgt,emb_weight))\n",
    "    \n",
    "    return(np.concatenate(samples_emb,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Precision & Recall\n",
    "import numpy as np\n",
    "from prdc import compute_prdc\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import BallTree \n",
    "import sklearn.metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def compute_nearest_neighbour_distances(input_features, nearest_k):\n",
    "\n",
    "    tree = BallTree(input_features,leaf_size = 40,metric='l1')\n",
    "    dist, _ = tree.query(input_features, k=(nearest_k+1)) \n",
    "    radii = dist[:,nearest_k]\n",
    "    return radii\n",
    "\n",
    "def compute_pairwise_distance(data_x, data_y=None):\n",
    "\n",
    "    if data_y is None:\n",
    "        data_y = data_x\n",
    "    dists = sklearn.metrics.pairwise_distances(\n",
    "        data_x, data_y, metric='l1', n_jobs=-1)\n",
    "    return dists\n",
    "\n",
    "\n",
    "# def compute_pairwise_distance(data_x, data_y=None):\n",
    "\n",
    "#     if data_y is None:\n",
    "#         data_y = data_x\n",
    "#     dists = cdist(data_x, data_y, metric='euclidean')\n",
    "#     return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(x_population)\n",
    "\n",
    "x_pop_dum = enc.transform(x_population).toarray()\n",
    "x_gen_dum = enc.transform(resamples).toarray()\n",
    "x_sam_dum = enc.transform(x_sample).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "\n",
    "pop_embed = convert_to_embedding(x_pop_dum)\n",
    "gen_embed = convert_to_embedding(x_gen_dum)\n",
    "sam_embed = convert_to_embedding(x_sam_dum)\n",
    "\n",
    "pop_features = pop_embed[np.random.choice(pop_embed.shape[0], size=100000, replace=False)]\n",
    "gen_features = gen_embed[np.random.choice(gen_embed.shape[0], size=5000, replace=False)]\n",
    "sam_features = sam_embed[np.random.choice(sam_embed.shape[0], size=5000, replace=False)]\n",
    "\n",
    "results =[]\n",
    "for k in [3,5,7]:\n",
    "    metrics_gen = compute_prdc(pop_features,gen_features,k)\n",
    "    metrics_sam = compute_prdc(pop_features,sam_features,k)\n",
    "    results.append(np.concatenate([metrics_gen,metrics_sam],axis=0))\n",
    "    \n",
    "outputs = pd.DataFrame(np.concatenate(np.array(results),axis=0).reshape(-1,8))\n",
    "outputs.to_clipboard()\n",
    "\n",
    "et = time.time()\n",
    "print(et-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STZ = []\n",
    "SAZ = []\n",
    "\n",
    "\n",
    "np.sum(p_gen.isin(p_pop))/len(p_gen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p_sam.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "### SC data\n",
    "\n",
    "samples_train =  wide_to_long(x_train_cond)\n",
    "\n",
    "## OOS Ratio\n",
    "resamples_pvalue = resamples_SC.iloc[:,0].astype(str)\n",
    "for i in range(resamples_SC.shape[1]-1):\n",
    "    resamples_pvalue = resamples_pvalue+resamples_SC.iloc[:,(i+1)].astype(str)\n",
    "    \n",
    "samples_pvalue = samples.iloc[:,0].astype(str)\n",
    "for i in range(samples.shape[1]-1):\n",
    "    samples_pvalue = samples_pvalue+samples.iloc[:,(i+1)].astype(str)\n",
    "\n",
    "resamples_pvalue_ind = resamples_SC.iloc[:,0].astype(str)\n",
    "for i in range(resamples_SC.shape[1]-6):\n",
    "    resamples_pvalue_ind  = resamples_pvalue_ind +resamples_SC.iloc[:,(i+1)].astype(str)\n",
    "    \n",
    "samples_pvalue_ind = samples.iloc[:,0].astype(str)\n",
    "for i in range(samples.shape[1]-6):\n",
    "    samples_pvalue_ind  = samples_pvalue_ind +samples.iloc[:,(i+1)].astype(str)\n",
    "    \n",
    "    \n",
    "OOS_resam = resamples_SC.shape[0]-np.sum(resamples_pvalue.isin(samples_pvalue))\n",
    "OOS_ratio = OOS_resam/resamples_SC.shape[0]\n",
    "OOS_resam_ind = resamples_SC.shape[0]-np.sum(resamples_pvalue_ind.isin(samples_pvalue_ind))\n",
    "OOSI_ratio_ind = OOS_resam_ind/resamples_SC.shape[0]\n",
    "\n",
    "\n",
    "## CZ Ratio\n",
    "C1 = sum(resamples_SC['TP_0'] == 'TP_0_Z')\n",
    "C2 = sum((resamples_SC['TP_1'] == 'TP_1_Z'))\n",
    "C3 = sum((resamples_SC['TP_2'] == 'TP_2_Z') & ((resamples_SC['TP_3'] != 'TP_3_Z') | (resamples_SC['TP_4'] != 'TP_4_Z')))\n",
    "C4 = sum((resamples_SC['TP_3'] == 'TP_3_Z') & ((resamples_SC['TP_4'] != 'TP_4_Z')))\n",
    "CZ = C1+C2+C3+C4\n",
    "print(OOS_ratio)\n",
    "print(OOSI_ratio_ind)\n",
    "print(CZ/OOS_resam)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
